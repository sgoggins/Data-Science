?plot
?abline
?plot
install.packages("statnet")
#!/usr/bin/env Rscript
#library(ggplot2)
{
?plot
}
setwd("~/GitHub/info480")
#!/usr/bin/env Rscript
## Look at the networksis package for this data...
## Quite a gap between methods used in journals and the methods that
## are available for use.  R plays a role
## TODO - GOOGLE VIS in R
rm(list=ls(all=TRUE))
library(network)
library(statnet)
########
## Other Libraries with Utility for network analysis
########
#library(tnet)
#library(igraph)
#library(lsa)
#library(RODBC)
#library(ggplot2)
#library(blockmodeling)
#igraph.par("print.vertex.attributes", TRUE)
#igraph.par("print.edge.attributes", TRUE)
graphSetArray <- c()
graphPrefixArray <- c()
inDegArray <- c()
outDegArray <- c()
betweennessArray <- c()
labelArray <- c()
nodeArray <- c()
####################################################################
# Read the Data in
####################################################################
gitLists <- system("dir project*edgelist.csv", intern=T)
for (i in 1:length(gitLists))
{
filename <- gitLists[i]
el <- read.csv(filename, header=TRUE, row.names=NULL)
disAll=network(el,matrix.type="edgelist")
centered <- centralization(disAll, degree, normalize)
ideg <- degree(disAll, cmode="indegree")
odeg <- degree(disAll, cmode="outdegree")
set.vertex.attribute(disAll, "indegree", degree(disAll, cmode="indegree"))
set.vertex.attribute(disAll, "outdegree", degree(disAll, cmode="outdegree"))
viz1=paste("output/",filename,"viz1",i,".png")
png(viz1)
plot(disAll, displayisolates = FALSE, vertex.col="blue", edge.len=(1/n$mel.dist))
dev.off()
viz2=paste("output/",filename,"viz2",i,".png")
png(viz2)
gplot(disAll,vertex.cex=(ideg+odeg)^0.5/2, vertex.sides=50, label.cex=0.8, vertex.col=rgb(odeg/max(odeg), 0, ideg/max(ideg)), label=network.vertex.names(disAll), displayisolates=FALSE,  boxed.labels=FALSE)
gplot(disAll,vertex.cex=(ideg+odeg)^0.2/2, vertex.sides=50, label.cex=0.8, 	vertex.col=rgb(odeg/max(odeg), 0, ideg/max(ideg)),  displayisolates=FALSE)
dev.off()
viz3=paste("output/",filename,"viz3",i,".png")
png(viz3)
hist(ideg, xlab="Indegree", main="Indegree Distribution", prob=TRUE)
hist(odeg, xlab="Outdegree", main="Outdegree Distribution", prob=TRUE)
hist(odeg+ideg, xlab="Total degree", main="Total degree Distribution", prob=TRUE)
plot(ideg, odeg, type="n", xlab="incoming", ylab="outgoing")
abline(0,1, lty=3)
text(jitter(ideg), jitter(odeg), network.vertex.names(disAll), cex=0.75, col=2)
dev.off()
viz4=paste("output/", filename, "nameline", i, ".png")
png(viz4)
plot(ideg, odeg, type="n", xlab="incoming", ylab="outgoing", main=centered)
abline(0,1, lty=3)
text(jitter(ideg), jitter(odeg), network.vertex.names(disAll), cex=2.0, col=1)
dev.off()
}
#
#  This code does not work right now; and its not needed.  Derived from another program used a while ago.
#
# 	V(disAll)$ideg<-degree(disAll, mode="in")
# 	V(disAll)$odeg<-degree(disAll, mode="out")
# 	V(disAll)$BTW<-signif(betweenness(disAll), digits=2)
# 	labelTwo=paste(V(disAll)$name, "\n", " IN=", V(disAll)$ideg, " OUT=", V(disAll)$odeg,"\n", "Betweenness=",V(disAll)$BTW)
# 	labelThree=paste(V(disAll)$name,",",V(disAll)$ideg,",", V(disAll)$odeg,",", V(disAll)$BTW,",")
# 	#V(disAll)$label <- V(disAll)$name
# 	V(disAll)$label <- labelTwo
# 	V(disAll)$nodeSummary <- labelThree
# 	#print(V(disAll)$label)
# 	labelArray <<- rbind(labelArray, paste(V(disAll)$nodeSummary, collapse=" "))
#
#
# 	modeg=max(V(disAll)$odeg)
# 	mideg=max(V(disAll)$ideg)
# 	E(disAll)$width <- E(disAll)$w
# 	E(disAll)$arrow.size <-0.99
# 	E(disAll)$edgeBetween <- edge.betweenness(disAll)
#E(disAll)$label <- paste("edge betweenness","\n", E(disAll)$edgeBetween)
# 	#layout.spring(disAll,weights=E(disAll)$weight+.01)
# 	V(disAll)$size <-20^((V(disAll)$ideg+V(disAll)$odeg)/(modeg+mideg))
# 	V(disAll)$color <- rgb(V(disAll)$odeg/modeg, 0, V(disAll)$ideg/mideg)
#
fix(betweennessArray)
setwd("~/GitHub/info480")
## Always a good practice to release old data from memory
rm(list=ls(all=TRUE))
### stringr is a string processing package for R
if(is.installed("stringr")==FALSE)
install.packages("stringr",repos="http://cran.us.r-project.org")
else
print("library installed.  yay!")
# Load the libraries you need
library(stringr)
####This file assigns the name of your file which is in quotes to inputfile
####I use this syntax in case I want to read in a lot of files at one
#inputfile <- "londonweek2.csv"
inputfile <- "londonsample.csv"
####The following piece of code reads the file which was identified above into a dataframe
####In R, dataframes are one of the foundational data structures that are similar to a ####spreadsheet
df <- read.csv(file=inputfile)
###examine the dataframe that is read in
head(df)
###assign the number of rows in df to the numberoftweets variable.
numberoftweets<-nrow(df)
########################################################
########################################################
########################################################
########################################################
####create a new field for aggregate analysis by day
###the following line strips away the hour:minutes:second of created_at
df$created_day<-as.Date(df$created_at, format="%m/%d/%Y")
########################################################
########################################################
########################################################
########################################################
########################################################
####create a table of tweets by day for basic longitudinal analysis
timeseries<-table(df$created_day)
#####Basic plot of the frequencies of tweets on a daily basis
#plot(timeseries)
###typing in number of tweets will display the number of tweets
numberoftweets
###make text lowercase
df$text<-tolower(df$text)
###make all usernames lowercase
df$from_user<-tolower(df$from_user)
####Using str_extract function (part of the stringr library), pull out any username that is ####at the beginning of a tweet
df$to= str_extract(df$text,"^(@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is ####at the beginning of a tweet, preceded by a period (more on this in another post)
df$fauxto= str_extract(df$text,"^(.@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is
####preceded by a rt which would signify a retweet
df$retweet=str_extract(df$text,"rt (@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is
####preceded by a via which would signify another form of a retweet
df$via= str_extract(df$text, "via (@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is
####preceded by a mt which would signify a modified tweet
df$modifiedtweet=str_extract(df$text,"mt (@[[:graph:]_]*)")
####This identifies the presence of a link and assigns that value by row to linkpresence
df$linkpresence=str_detect(df$text, '(http://[[:graph:]\\s]*)')
#### This identifies the presence of a fauxto (more later) and assigns that value by row to ####fauxtopresence
df$fauxtopresence=str_detect(df$text, "^(.@[[:graph:]_]*)")
####This identifies the presence of a hashtag and assigns that value by row to ####hashpresence
df$hashpresence=str_detect(df$text, "(#[[:graph:]_]*)")
####This identifies the presence of an @-mention and assigns that value by row to #####mention presence
df$mentionpresence = str_detect(df$text, "(@[[:graph:]_]*)")
#########This identifies the presence of an at-reply and assigns that value by row to #####replytopresence
df$replytopresence = str_detect(df$text, "^(@[[:graph:]_]*)")
####This identifies the presence of a retweet and assigns that value by row to rtpresence
df$rtpresence = str_detect(df$text, "rt (@[[:graph:]_]*)")
####create variable filename
filename=paste(inputfile, "MARKEDUP.csv")
####write file to a csv
write.csv(df, file=filename)
#####make a table of the str_detect from the previous
linkpresence=table(df$linkpresence)
####sum the number of true
linktruecount <- sum(df$linkpresence == 'TRUE')
####sume the number of false
linkfalsecount <- sum(df$linkpresence == 'FALSE')
####identify the percentage of links in the dataset
linkpresencepercentage <- (linktruecount/(linktruecount+linkfalsecount))
#write.csv(linkpresence, file="linkpresence.csv")
####rinse and repeat
fauxtopresence=table(df$fauxtopresence)
fauxtotruecount <- sum(df$fauxtopresence == 'TRUE')
fauxtofalsecount <- sum(df$fauxtopresence == 'FALSE')
fauxtopresencepercentage <- (fauxtotruecount/(fauxtotruecount+fauxtofalsecount))
hashpresence=table(df$hashpresence)
hashtruecount <- sum(df$hashpresence == 'TRUE')
hashfalsecount <- sum(df$hashpresence == 'FALSE')
hashpresencepercentage <- (hashtruecount/(hashtruecount+hashfalsecount))
write.csv(hashpresence, file="hashpresence.csv")
mentionpresence=table(df$mentionpresence)
mentiontruecount <- sum(df$mentionpresence == 'TRUE')
mentionfalsecount <- sum(df$mentionpresence == 'FALSE')
mentionpresencepercentage <- (mentiontruecount/(mentiontruecount+mentionfalsecount))
write.csv(mentionpresence, file="mentionpresence.csv")
replytopresence=table(df$replytopresence)
replytotruecount <- sum(df$replytopresence == 'TRUE')
replytofalsecount <- sum(df$replytopresence == 'FALSE')
replytopresencepercentage <- (replytotruecount/(replytotruecount+replytofalsecount))
write.csv(replytopresence, file="replytopresence.csv")
rtpresence=table(df$rtpresence)
rttruecount <- sum(df$rtpresence == 'TRUE')
rtfalsecount <- sum(df$rtpresence == 'FALSE')
rtpresencepercentage <- (rttruecount/(rttruecount+rtfalsecount))
write.csv(rtpresence, file="rtpresence.csv")
####create a table of the syntactical feature distribution we just examined
syntacticfeatureoverview <- table(inputfile, numberoftweets, linkpresencepercentage, hashpresencepercentage, mentionpresencepercentage, replytopresencepercentage, rtpresencepercentage)
filename=paste("syntacticfeatureoverview", inputfile)
write.csv(syntacticfeatureoverview, file = filename, row.names=TRUE)
###trim the @ sign from the columms where we pulled them out for easy analysis
trimat <- function (x) sub('@','',x)
df$to<-trimat(df$to)
df$retweet<-trimat(df$retweet)
df$fauxto<-trimat(df$fauxto)
df$via<-trimat(df$via)
df$modifiedtweet<-trimat(df$modifiedtweet)
###trim the rt from the retweet column
trimRT <- function (x) sub('rt ','',x)
df$retweet<-trimRT(df$retweet)
###trim the colon from the retweet column
trimcolon <- function (x) sub(':','',x)
df$retweet<-trimcolon(df$retweet)
###trim the period from the fauxto
trimperiod <- function (x) sub('\\.','',x)
df$fauxto<-trimperiod(df$fauxto)
###trim the via from the via column
trimVIA <- function (x) sub('via ','',x)
df$via<-trimVIA(df$via)
###trim the mt from the modifiedtweet column
trimMT <- function (x) sub('MT ','',x)
df$modifiedtweet<-trimMT(df$modifiedtweet)
###create a table of people that were retweeted
retweettable<-table(df$retweet)
####identify those that have been retweeted more than 500 times
highretweet<-subset(retweettable, retweettable>500)
write.csv(highretweet, file="highretweet.csv")
install.packages("strinr")
install.packages("stringr")
install.packages("stringr")
## Always a good practice to release old data from memory
rm(list=ls(all=TRUE))
library(stringr)
####This file assigns the name of your file which is in quotes to inputfile
####I use this syntax in case I want to read in a lot of files at one
#inputfile <- "londonweek2.csv"
inputfile <- "londonsample.csv"
####The following piece of code reads the file which was identified above into a dataframe
####In R, dataframes are one of the foundational data structures that are similar to a ####spreadsheet
df <- read.csv(file=inputfile)
###examine the dataframe that is read in
head(df)
###assign the number of rows in df to the numberoftweets variable.
numberoftweets<-nrow(df)
########################################################
########################################################
########################################################
########################################################
####create a new field for aggregate analysis by day
###the following line strips away the hour:minutes:second of created_at
df$created_day<-as.Date(df$created_at, format="%m/%d/%Y")
########################################################
########################################################
########################################################
########################################################
########################################################
####create a table of tweets by day for basic longitudinal analysis
timeseries<-table(df$created_day)
#####Basic plot of the frequencies of tweets on a daily basis
#plot(timeseries)
###typing in number of tweets will display the number of tweets
numberoftweets
###make text lowercase
df$text<-tolower(df$text)
###make all usernames lowercase
df$from_user<-tolower(df$from_user)
####Using str_extract function (part of the stringr library), pull out any username that is ####at the beginning of a tweet
df$to= str_extract(df$text,"^(@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is ####at the beginning of a tweet, preceded by a period (more on this in another post)
df$fauxto= str_extract(df$text,"^(.@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is
####preceded by a rt which would signify a retweet
df$retweet=str_extract(df$text,"rt (@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is
####preceded by a via which would signify another form of a retweet
df$via= str_extract(df$text, "via (@[[:graph:]_]*)")
#### Using str_extract function (part of the stringr library), pull out any username that is
####preceded by a mt which would signify a modified tweet
df$modifiedtweet=str_extract(df$text,"mt (@[[:graph:]_]*)")
####This identifies the presence of a link and assigns that value by row to linkpresence
df$linkpresence=str_detect(df$text, '(http://[[:graph:]\\s]*)')
#### This identifies the presence of a fauxto (more later) and assigns that value by row to ####fauxtopresence
df$fauxtopresence=str_detect(df$text, "^(.@[[:graph:]_]*)")
####This identifies the presence of a hashtag and assigns that value by row to ####hashpresence
df$hashpresence=str_detect(df$text, "(#[[:graph:]_]*)")
####This identifies the presence of an @-mention and assigns that value by row to #####mention presence
df$mentionpresence = str_detect(df$text, "(@[[:graph:]_]*)")
#########This identifies the presence of an at-reply and assigns that value by row to #####replytopresence
df$replytopresence = str_detect(df$text, "^(@[[:graph:]_]*)")
####This identifies the presence of a retweet and assigns that value by row to rtpresence
df$rtpresence = str_detect(df$text, "rt (@[[:graph:]_]*)")
####create variable filename
filename=paste(inputfile, "MARKEDUP.csv")
####write file to a csv
write.csv(df, file=filename)
#####make a table of the str_detect from the previous
linkpresence=table(df$linkpresence)
####sum the number of true
linktruecount <- sum(df$linkpresence == 'TRUE')
####sume the number of false
linkfalsecount <- sum(df$linkpresence == 'FALSE')
####identify the percentage of links in the dataset
linkpresencepercentage <- (linktruecount/(linktruecount+linkfalsecount))
#write.csv(linkpresence, file="linkpresence.csv")
####rinse and repeat
fauxtopresence=table(df$fauxtopresence)
fauxtotruecount <- sum(df$fauxtopresence == 'TRUE')
fauxtofalsecount <- sum(df$fauxtopresence == 'FALSE')
fauxtopresencepercentage <- (fauxtotruecount/(fauxtotruecount+fauxtofalsecount))
hashpresence=table(df$hashpresence)
hashtruecount <- sum(df$hashpresence == 'TRUE')
hashfalsecount <- sum(df$hashpresence == 'FALSE')
hashpresencepercentage <- (hashtruecount/(hashtruecount+hashfalsecount))
write.csv(hashpresence, file="hashpresence.csv")
mentionpresence=table(df$mentionpresence)
mentiontruecount <- sum(df$mentionpresence == 'TRUE')
mentionfalsecount <- sum(df$mentionpresence == 'FALSE')
mentionpresencepercentage <- (mentiontruecount/(mentiontruecount+mentionfalsecount))
write.csv(mentionpresence, file="mentionpresence.csv")
replytopresence=table(df$replytopresence)
replytotruecount <- sum(df$replytopresence == 'TRUE')
replytofalsecount <- sum(df$replytopresence == 'FALSE')
replytopresencepercentage <- (replytotruecount/(replytotruecount+replytofalsecount))
write.csv(replytopresence, file="replytopresence.csv")
rtpresence=table(df$rtpresence)
rttruecount <- sum(df$rtpresence == 'TRUE')
rtfalsecount <- sum(df$rtpresence == 'FALSE')
rtpresencepercentage <- (rttruecount/(rttruecount+rtfalsecount))
write.csv(rtpresence, file="rtpresence.csv")
####create a table of the syntactical feature distribution we just examined
syntacticfeatureoverview <- table(inputfile, numberoftweets, linkpresencepercentage, hashpresencepercentage, mentionpresencepercentage, replytopresencepercentage, rtpresencepercentage)
filename=paste("syntacticfeatureoverview", inputfile)
write.csv(syntacticfeatureoverview, file = filename, row.names=TRUE)
###trim the @ sign from the columms where we pulled them out for easy analysis
trimat <- function (x) sub('@','',x)
df$to<-trimat(df$to)
df$retweet<-trimat(df$retweet)
df$fauxto<-trimat(df$fauxto)
df$via<-trimat(df$via)
df$modifiedtweet<-trimat(df$modifiedtweet)
###trim the rt from the retweet column
trimRT <- function (x) sub('rt ','',x)
df$retweet<-trimRT(df$retweet)
###trim the colon from the retweet column
trimcolon <- function (x) sub(':','',x)
df$retweet<-trimcolon(df$retweet)
###trim the period from the fauxto
trimperiod <- function (x) sub('\\.','',x)
df$fauxto<-trimperiod(df$fauxto)
###trim the via from the via column
trimVIA <- function (x) sub('via ','',x)
df$via<-trimVIA(df$via)
###trim the mt from the modifiedtweet column
trimMT <- function (x) sub('MT ','',x)
df$modifiedtweet<-trimMT(df$modifiedtweet)
###create a table of people that were retweeted
retweettable<-table(df$retweet)
####identify those that have been retweeted more than 500 times
highretweet<-subset(retweettable, retweettable>500)
write.csv(highretweet, file="highretweet.csv")
inputfile <- "londonsample.csv"
####The following piece of code reads the file which was identified above into a dataframe
####In R, dataframes are one of the foundational data structures that are similar to a ####spreadsheet
df <- read.csv(file=inputfile)
df <- read.csv(file=inputfile)
setwd("~/GitHub/info480/week2")
inputfile <- "londonsample.csv"
####The following piece of code reads the file which was identified above into a dataframe
####In R, dataframes are one of the foundational data structures that are similar to a ####spreadsheet
df <- read.csv(file=inputfile)
###examine the dataframe that is read in
head(df)
